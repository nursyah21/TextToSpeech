{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wav2vec\n",
    "source: https://huggingface.co/facebook/wav2vec2-base-960h\n",
    "paper: https://arxiv.org/abs/2006.11477\n",
    "\n",
    "hasil ujicoba dengan wav2vec2 dirasa kurang memuaskan dikarenakan pada pengujian yang menggunakan rekaman suara yang berbeda tidak menunjukkan setidaknya satu kata yang benar\n",
    "namun wav2vec2 masih bisa dijelajah lebih lanjut \n",
    "\n",
    "alternatif berikutnya akan mencoba menggunakan whisper\n",
    "source: https://github.com/openai/whisper\n",
    "paper: https://cdn.openai.com/papers/whisper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Found cached dataset librispeech_asr_dummy (/home/nursyah/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import io\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "# load model and tokenizer\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    \n",
    "# load dummy dataset and read soundfiles\n",
    "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1():\n",
    "    path_audio = ds[0]['audio']['path']\n",
    "    # tokenize\n",
    "    input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values  # Batch size 1\n",
    "\n",
    "    # retrieve logits\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "    # take argmax and decode\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "    song = AudioSegment.from_file(path_audio, format='flac')\n",
    "    print(f\"playing sound using  pydub\")\n",
    "    play(song)\n",
    "    print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2():\n",
    "    path_audio = './yt2.wav'\n",
    "\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(path_audio) as source:\n",
    "        audio = r.record(source)\n",
    "        data = io.BytesIO(audio.get_wav_data())\n",
    "        clip = AudioSegment.from_file(data)\n",
    "        x = torch.FloatTensor(clip.get_array_of_samples())\n",
    "\n",
    "        # processing\n",
    "        inputs = processor(x, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values \n",
    "        logits = model(inputs).logits\n",
    "\n",
    "        # take argmax and decode\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids)\n",
    "        print(transcription)\n",
    "\n",
    "        \n",
    "        print(path_audio)\n",
    "        song = AudioSegment.from_file(path_audio)\n",
    "        print(f\"playing sound using  pydub\")\n",
    "        play(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HONO LACHAMMM']\n",
      "./yt2.wav\n",
      "playing sound using  pydub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n"
     ]
    }
   ],
   "source": [
    "test2()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57f56da1b9090ce9eccf4a4acd48bcd74edf7b57b483b6523c299becc1e1959d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('TextToSpeech')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
